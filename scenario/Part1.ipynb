{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11dfb974",
   "metadata": {},
   "source": [
    "## 직장인을 위한 파이썬 데이터 분석 초격차 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1307789",
   "metadata": {
    "id": "NUmCH3V5zYqI"
   },
   "source": [
    "\n",
    "## **문제해결 프로세스 정의**\n",
    "---\n",
    "> **문제정의**\n",
    "\n",
    "```\n",
    "▶ 신용카드 대금 채무 불이행으로 인한 손실\n",
    "```  \n",
    "\n",
    "> **기대효과**\n",
    "\n",
    "```\n",
    "▶ 채무 불이행으로 인한 손실 감소\n",
    "```\n",
    "\n",
    "> **해결방안**\n",
    "\n",
    "```\n",
    "▶ 채무 불이행 고객 예측 및 블랙리스트 관리(신용한도 조정)\n",
    "▶ Session 1 🥉\n",
    " - 고객 프로필 정보 확인 및 파생변수 생성 및 검증\n",
    "▶ Session 2 🥈\n",
    " - 고객 프로필 정보, 이용한도에 따른 채무 불이행률 탐색\n",
    "▶ Session 3 🥇\n",
    " - ML알고리즘 활용 채무 불이행 고객 예측\n",
    "```\n",
    "\n",
    "> **성과측정**  \n",
    "\n",
    "```\n",
    "▶ 블랙리스트 관리 후 채무 불이행 손실 감소률(%)\n",
    "```\n",
    "\n",
    "> **현업적용**  \n",
    "\n",
    "```\n",
    "▶ 매 월 Model에 Input하기 위한 Data mart 생성\n",
    "▶ Data mart 예측 Model에 Input후 채무불이행 예측 고객군 추출\n",
    "▶ 추출한 고객 대상으로 신용한도 조정 및 블랙리스트 관리를 통한 손실 방어\n",
    "```\n",
    "\n",
    "> **주요 코드 미리 살펴보기**  \n",
    "\n",
    "```\n",
    "▶ session 1 → np.where(), sns.catplot(), replace()\n",
    "▶ session 2 → value_counts(), d.pivot_table(), reset_index(), sort_values()\n",
    "▶ session 3 → RandomForestClassifier, roc_auc_score()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트를 설명할 때 Flow를 잘 정리해야 한다.\n",
    "# 위처럼 문제정의, 해결방안, 기대효과, 성과측정, 현업적용 순으로 설명해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e644993",
   "metadata": {},
   "source": [
    "#### [Part2 - CH03] Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf161f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가적으로 옵션을 조사해야 한다.\n",
    "# ----- 지도 -----\n",
    "# folium.Figure(width, height)\n",
    "# folium.Map(location=[위도,경도], zoom_start, tiles)\n",
    "# folium.choropleth(geo_data = seoul_json, fill_color)\n",
    "# ----- 마커 -----\n",
    "# 마커\n",
    "'''\n",
    "folium.Marker([37.50327278745055, 127.04160945554442],\n",
    "               popup = '패스트 캠퍼스',     # 마커 이름 입력\n",
    "               icon = folium.Icon(color='blue')).add_to(map)\n",
    "'''\n",
    "# 원형 마커\n",
    "'''\n",
    "folium.CircleMarker([37.50327278745055, 127.04160945554442],\n",
    "                     radius = 100,    # 범위\n",
    "                     color = 'skyblue',\n",
    "                     fill_color = 'skyblue',\n",
    "                     popup = '패스트 캠퍼스',     # 마커 이름 입력\n",
    "                     icon = folium.Icon(color='blue')).add_to(map)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39883f0",
   "metadata": {},
   "source": [
    "#### [Part2 - CH04 - 09] HyperParameter Tunning\n",
    "설정 가능한 Hyper Parameter를 조정하며 모델 성능을 최적화하는 과정이다.<br>\n",
    "$\\bullet$ GridSearchCV\n",
    "- 지정한 HyperParameter와 값들을 격자 형식으로 조합해 모두 탐색한 뒤 최고 성능인 조합을 반환한다.\n",
    "- 많은 수의 HyperParameter와 값들을 탐색할 때 오래 걸린다.\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search =  GridSearchCV(모델, 하이퍼파라미터DICT, cv=FOLD수, scoring=평가지표)\n",
    "grid_search.fit(X_train, y_train)\n",
    "# 최고 성능 파라미터\n",
    "grid_search.best_params_\n",
    "```\n",
    "\n",
    "$\\bullet$ RandomizedSearchCV\n",
    "- 지정한 HyperParameter와 값들을 격자 형식으로 조합한 뒤 지정한 횟수만큼 무작위 탐색해 최고 성능인 조합을 반환한다.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(모델, 하이퍼파라미터DICT, n_iter=탐색수, cv=FOLD수, scoring=평가지표)\n",
    "random_search.fit(X_train, y_train)\n",
    "# 최고 성능 파라미터\n",
    "random_search.best_params_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b351f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaysianOptimization 설명을 추가해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe295fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "lgbm_model = LGBMRegressor()\n",
    "\n",
    "# 하이퍼파라미터DICT 정의\n",
    "# lgbm_model.get_params()\n",
    "# 학습률(틀린 값에 대한 가중치), 나뭇잎 수, 나무를 만드는데 사용할 최소 데이터수, 부스팅 횟수(나무수)\n",
    "param = {'learning_rate ': [0.05, 0.1],\n",
    "         'num_leaves' : [31, 50],\n",
    "         'min_child_samples' : [50],\n",
    "         'n_estimators': [100, 300]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch 수행\n",
    "grid_search = GridSearchCV(lgbm_model, param, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# 최적 파라미터 확인\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1040a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch 수행\n",
    "random_search = RandomizedSearchCV(lgbm_model, param, n_iter=10, cv=5, scoring='neg_mean_squared_error')\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# 최적 파라미터 확인\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dad82a",
   "metadata": {},
   "source": [
    "#### [Part2 - CH04 - 10] Clustering\n",
    "$\\bullet$ Kmeans\n",
    "- 유클리디안 거리를 기반으로 가까운 데이터끼리 묶는다.\n",
    "- `Inertia지수`가 급격히 떨어지는 지점을 보통 적정 군집수로 한다.(Elbow Method)\n",
    "- Inertia지수는 각 클러스터의 중심에서 클러스터에 할당된 데이터 포인트간 거리를 합산한 것이다. 지수의 값이 작을수록 응집된 정도가 높다고 평가한다.\n",
    "\n",
    "$\\bullet$ DBSCAN\n",
    "- 밀집성이 높은 부분끼리 군집을 나눈다.\n",
    "- 특정 데이터의 `epilon반경` 내 최소 포인트(minPts) 이상 데이터가 있으면 같은 군집으로 간주하고 미만이면 군집 경계선으로 삼는다.<br>\n",
    "  두 조건에 부합하지 않는 데이터는 Noise Point로 한다.\n",
    "- 군집 수를 지정할 필요가 없으며 비선형, 불균일 데이터에 유용하다.\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=EPILON반경, min_samples=MINPTS, metric=거리지표)\n",
    "dbscan.fit(TRAIN)\n",
    "# 군집화 결과 확인\n",
    "dbscan.labels_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA 설명을 추가해야 한다.\n",
    "# 실루엣 계수의 설명을 추가해야 한다.\n",
    "# DBSCAN 작동 원리를 추가적으로 조사해야 한다.\n",
    "\n",
    "# 3D 시각화\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단위를 맞춰주는 스케일링 예시를 들어야 한다.\n",
    "# 정규화(Normalization, MinMaxScaler)란 일정 구간에 있도록 변환(압축)하는 작업을 의미한다. \n",
    "# 자릿수가 늘어나지 않아 연산 오버플로우를 방지할 수 있다.\n",
    "# 표준화는 feature간 단위 차이가 극심하게 나는 상황에서 더 유리하다. 또 비지도 학습(데이터 범위에 무지한 상황)일 때도 선호된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccced67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
